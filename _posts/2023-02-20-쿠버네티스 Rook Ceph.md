---
layout: article
title: "[Kubernetes] rook-ceph node 생성 에러해결하기 "
subtitle: "쿠버네티스 rook-ceph 에러해결하기"
date: 2023-02-20 16:05:00 +0900
lastmod: 2023-02-20 20:50:00 +0900
tags: 
  - 리눅스
  - k8s

---

<!--more-->  
윈도우(Windows11)환경의 VirtualBox에 설치한 우분투 서버이미지(20.04 LTS)에서 진행하였습니다. 마스터(1대) 워커노드(3대) 로 진행하였습니다.<br/>


쿠버네티스 rook-ceph 에러해결하기<br/>

rook-ceph 에러 해결하기<br/>

rook-ceph 로 클러스터링 구성을 하다 발생한 문제를 해결한 글이다.

# OSD-Node 생성 오류

rook-ceph로 클러스터링 구성 후 삭제한 다음 다시 구성하니 `rook-ceph-osd-node` 가 생성 되지 않는 문제가 발생했다.<br/>

![스크린샷_20230220_055915](https://user-images.githubusercontent.com/99805929/220062813-2a4cee82-4dfa-40b1-b54e-e309cf0e093a.png)<br/>

## cluster 삭제하기

처음에 [공식문서](https://rook.io/docs/rook/v1.10/Getting-Started/quickstart/#prerequisites)에 적힌대로 git clone 후 create 명령어로 실행을 했고 삭제 할 때에 `cluster.yaml` 파일을 나중에 삭제하는 경우 namespace 오류가 발생하는 것 같으니 `cluster.yaml` 파일을 먼저 삭제해주자.<br/>

```javascript
$ git clone --single-branch --branch v1.10.11 https://github.com/rook/rook.git
cd rook/deploy/examples
kubectl create -f crds.yaml -f common.yaml -f operator.yaml
kubectl create -f cluster.yaml

# 삭제 시 사용한 명령어 (생성한 순서의 역순으로 삭제)
kubectl delete -f cluster.yaml
kubectl delete -f crds.yaml -f common.yaml -f operator.yaml
```

이 상태에서 다시 생성을하면 node가 생성되지 않는 문제가 발생했다.<br/>

## 각 Node의 rook 파일 삭제

각 노드에 접속해서 `/var/lib/rook` 디렉토리를 삭제해줍니다.

```javascript
# 설치 방식에 따라 노드 접속방법은 다를 수 있다.

ssh node1
sudo rm -rf /var/lib/rook
```

## CleanUP
이러고 다시 yaml 파일을 실행 해서 클러스터 구성을 해보았는데 osd-node는 생성이 되었으나 볼륨 연결이 되지 않는 문제가 발생했다.<br/>

내가 모르는 어딘가에 파일이 남아있거나 찾기 어려워 공식문서의 [다음 내용](https://rook.io/docs/rook/v1.10/Getting-Started/ceph-teardown/?h=cleanup#delete-the-block-and-file-artifacts)을 처음부터 끝까지 따라했다.<br/>

```javascript
# CleanUP

kubectl -n rook-ceph patch cephcluster rook-ceph --type merge -p '{"spec":{"cleanupPolicy":{"confirmation":"yes-really-destroy-data"}}}'

kubectl -n rook-ceph delete cephcluster rook-ceph

kubectl -n rook-ceph get cephcluster

kubectl delete -f operator.yaml
kubectl delete -f common.yaml
kubectl delete -f psp.yaml
kubectl delete -f crds.yaml

# 안되는 경우 sudo 명령어 붙여주기

DISK="/dev/sdX"
sgdisk --zap-all $DISK
dd if=/dev/zero of="$DISK" bs=1M count=100 oflag=direct,dsync
blkdiscard $DISK
partprobe $DISK

ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %
rm -rf /dev/ceph-*
rm -rf /dev/mapper/ceph--*

for CRD in $(kubectl get crd -n rook-ceph | awk '/ceph.rook.io/ {print $1}'); do
    kubectl get -n rook-ceph "$CRD" -o name | \
    xargs -I {} kubectl patch -n rook-ceph {} --type merge -p '{"metadata":{"finalizers": []}}'
done

kubectl api-resources --verbs=list --namespaced -o name \
  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n rook-ceph


kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p '{"metadata":{"finalizers": []}}'
kubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p '{"metadata":{"finalizers": []}}'

```

## 볼륨 추가하기

각 노드별로 볼륨 인식은 되는데 rook-ceph 에서만 인식이 안되서 새로운 볼륨도 각 노드별로 추가해주었다.<br/>

![스크린샷_20230220_061101](https://user-images.githubusercontent.com/99805929/220063001-3a413fa2-8661-4e7a-8e74-fb1ea7204e35.png)


![스크린샷_20230220_061032](https://user-images.githubusercontent.com/99805929/220063396-3012fe9d-b20f-4678-8a24-79fead59674c.png)<br/>

![스크린샷_20230220_061632](https://user-images.githubusercontent.com/99805929/220063951-84c3aadd-0602-4182-b93f-917dd64da8f6.png)<br/>

각 노드별로 30GiB의 볼륨을 추가로 마운트해 주었는데 표시되는 볼륨 크기가 90GiB인걸 봐서는 기존에 있던 볼륨(25GiB)는 표시만 되고 인식이 안되는 것같다. 아마 인식이 된다면 55Gib x 3 해서 165GiB가 되야하지 않을까 싶다.




<br/>
<br/>

# Reference
[ceph osd 생성 안될 때](https://da-nika.tistory.com/179)<br/>
[CleanUP](https://rook.io/docs/rook/v1.10/Getting-Started/ceph-teardown/?h=cleanup)<br/>

